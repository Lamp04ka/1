{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeQWKRTpGU9s"
      },
      "source": [
        "# Лабораторная работа №1 \n",
        "## Вариант 4\n",
        "#### Волченков С.Г., Китель Б.В. 4230М   \n",
        "   \n",
        "      \n",
        "##Загрузка данных с Git уже без лишних параметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2a5NXPTo-5jM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "import numpy as np\n",
        "\n",
        "from openpyxl import load_workbook\n",
        "from io import BytesIO\n",
        "import urllib\n",
        "from openpyxl.styles import Font\n",
        "\n",
        "def load_workbook_from_url(url):\n",
        "    file = urllib.request.urlopen(url).read()\n",
        "    return load_workbook(filename = BytesIO(file))\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Lamp04ka/1/main/LB3V4.xlsx\"\n",
        "\n",
        "WorkBook = load_workbook_from_url(url)\n",
        "WorkSheet = WorkBook.active"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8sX10ZWGmFH"
      },
      "source": [
        "## Разметку данных по классам\n",
        "добавления признака меток классов. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "34BHTpOJAWmx"
      },
      "outputs": [],
      "source": [
        "def getRedRows(ds):\n",
        "  RedRows = []\n",
        "  for row in ds.iter_rows(max_col = ds.max_column - 1, min_row = 2, max_row = ds.max_row):\n",
        "    red = 0\n",
        "    for cell in row:\n",
        "      if(cell.font.color.rgb == \"FFFF0000\"):\n",
        "        red += 1\n",
        "        if(red >= 2):\n",
        "          RedRows.append(cell.row)\n",
        "          break\n",
        "  return RedRows\n",
        "\n",
        "def getClassRows(RedRows):\n",
        "  ClassRows = []\n",
        "  col = 1\n",
        "  for i in range(0, len(RedRows)-1):\n",
        "    if(RedRows[i]+1 == RedRows[i+1]):\n",
        "      col+=1\n",
        "    else:\n",
        "      if(col>3):\n",
        "        for j in range(i,i+col):\n",
        "          ClassRows.append(1)\n",
        "        i+=col-1\n",
        "        col=1\n",
        "      else:\n",
        "        for j in range(i,i+col):\n",
        "          ClassRows.append(2)\n",
        "        col=1\n",
        "  for i in range(len(RedRows)-col,len(RedRows)):\n",
        "    if(col>3):\n",
        "      ClassRows.append(1)\n",
        "    else:\n",
        "      ClassRows.append(2)\n",
        "  return ClassRows\n",
        "\n",
        "\n",
        "RedRows = getRedRows(WorkSheet)\n",
        "ClassRows = getClassRows(RedRows)\n",
        "\n",
        "for col in WorkSheet.iter_cols(min_col = WorkSheet.max_column, max_col = WorkSheet.max_column, min_row=2, max_row = WorkSheet.max_row):\n",
        "  for cell in col:\n",
        "    if(RedRows.count(cell.row) == 0):\n",
        "      cell.value = 0\n",
        "    else:\n",
        "      cell.value = ClassRows[RedRows.index(cell.row)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ZHqWeO0oTV"
      },
      "source": [
        "##создания pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vYq5amUHaBhO"
      },
      "outputs": [],
      "source": [
        "HeadersSheet = WorkSheet[1]\n",
        "header = []\n",
        "HeadersSheet[0].value = \"time\"\n",
        "for i in HeadersSheet:\n",
        "  header.append(i.value)\n",
        "data_s = list(WorkSheet.values)\n",
        "data_s = data_s[1:]\n",
        "\n",
        "data = pd.DataFrame(data_s, columns=header)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуальное сравнения разбиение на классы(id элиментов отличается от номера строки в exel на 1)"
      ],
      "metadata": {
        "id": "iqxctJf9sYJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Снимок экрана 2023-05-28 220023.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "XWTnjYyIuNwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![1](https://raw.githubusercontent.com/Lamp04ka/1/main/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202023-05-28%20220023.png)"
      ],
      "metadata": {
        "id": "nFU1tm31vcsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Lamp04ka/1/blob/main/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202023-05-28%20220023.png"
      ],
      "metadata": {
        "id": "w3-UU2dAvLxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.reindex(np.linspace(0,15,15,dtype=int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "K0b4VrKgnjWh",
        "outputId": "8ebc3d1e-a5db-46f4-af04-05fca1d1af14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  time  Ubs,V  Ibs,A  Isun,A  Ipt1,A  Ipt2,A  Ipt3,A  Ipt4,A  \\\n",
              "0  2014-05-02 00:00:31  14.63   0.27    0.13    0.09    0.09     0.0     0.0   \n",
              "1  2014-05-02 00:01:31  14.76    NaN     NaN     NaN    0.07     0.0     0.0   \n",
              "2  2014-05-02 00:02:31  13.64   0.24    0.72    0.09    0.09     0.0     0.0   \n",
              "3  2014-05-02 00:03:31  14.69   0.21    0.56    0.07    0.09     0.0     0.0   \n",
              "4  2014-05-02 00:04:31  14.76   0.21    0.83    0.07    0.07     0.0     0.0   \n",
              "5  2014-05-02 00:05:31  14.69   0.21    0.19    0.09    0.09     0.0     0.0   \n",
              "6  2014-05-02 00:06:31  14.76   0.24    0.91    0.07    0.07     0.0     0.0   \n",
              "7  2014-05-02 00:07:31  14.69   0.21    0.45    0.07    0.09     0.0     0.0   \n",
              "8  2014-05-02 00:08:31  14.69   0.21    0.53    0.09    0.07     0.0     0.0   \n",
              "9  2014-05-02 00:09:31  14.83   0.24    0.91    0.07    0.07     0.0     0.0   \n",
              "10 2014-05-02 00:10:31  14.69   0.21    0.21    2.87    0.07     0.0     0.0   \n",
              "11 2014-05-02 00:11:31  14.76   0.24    0.72    0.07    0.09     0.0     0.0   \n",
              "12 2014-05-02 00:12:31  14.76   0.24    0.59    0.09    0.07     0.0     0.0   \n",
              "13 2014-05-02 00:13:31  14.76   0.24    0.64    0.07    0.09     0.0     0.0   \n",
              "15 2014-05-02 00:15:31  14.69   0.24    0.11    0.07    0.07     0.0     0.0   \n",
              "\n",
              "    Ipt5,A  Ipt6,A  ...  TDS7,C  TDS8,C  TDS9,C  TKpt,C  TGbv,C  TNap,C  \\\n",
              "0      0.0     0.0  ...    15.0    15.0    15.0    24.0    16.0    16.0   \n",
              "1      0.0     0.0  ...     NaN     NaN    15.0    24.0    16.0    16.0   \n",
              "2      0.0     0.0  ...    16.0    15.0    15.0    24.0    16.0    16.0   \n",
              "3      0.0     0.0  ...    16.0    16.0    15.0    24.0    16.0    16.0   \n",
              "4      0.0     0.0  ...    16.0    16.0    15.0    24.0    16.0    16.0   \n",
              "5      0.0     0.0  ...    16.0    16.0    15.0    24.0    16.0    16.0   \n",
              "6      0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "7      0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "8      0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "9      0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "10     0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "11     0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "12     0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "13     0.0     0.0  ...    16.0    16.0    16.0    24.0    16.0    16.0   \n",
              "15     0.0     0.0  ...    17.0    17.0    16.0    24.0    16.0    16.0   \n",
              "\n",
              "    TPrd2,C  TPrd1,C  TDS24,C  Class  \n",
              "0      16.0     16.0     19.0      0  \n",
              "1       NaN     16.0     19.0      0  \n",
              "2      16.0     16.0     19.0      0  \n",
              "3      16.0     16.0     19.0      0  \n",
              "4      17.0     16.0     19.0      0  \n",
              "5      17.0     16.0     19.0      0  \n",
              "6      17.0     16.0     19.0      0  \n",
              "7      17.0     16.0     19.0      0  \n",
              "8      17.0     17.0     19.0      0  \n",
              "9      17.0     17.0     19.0      0  \n",
              "10     17.0     17.0     19.0      2  \n",
              "11     17.0     17.0     19.0      0  \n",
              "12     17.0     17.0     19.0      0  \n",
              "13     17.0     17.0     19.0      0  \n",
              "15     17.0     17.0     19.0      0  \n",
              "\n",
              "[15 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66a54b2b-88ff-4478-bc4e-cf423b8ac7fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>Ubs,V</th>\n",
              "      <th>Ibs,A</th>\n",
              "      <th>Isun,A</th>\n",
              "      <th>Ipt1,A</th>\n",
              "      <th>Ipt2,A</th>\n",
              "      <th>Ipt3,A</th>\n",
              "      <th>Ipt4,A</th>\n",
              "      <th>Ipt5,A</th>\n",
              "      <th>Ipt6,A</th>\n",
              "      <th>...</th>\n",
              "      <th>TDS7,C</th>\n",
              "      <th>TDS8,C</th>\n",
              "      <th>TDS9,C</th>\n",
              "      <th>TKpt,C</th>\n",
              "      <th>TGbv,C</th>\n",
              "      <th>TNap,C</th>\n",
              "      <th>TPrd2,C</th>\n",
              "      <th>TPrd1,C</th>\n",
              "      <th>TDS24,C</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-05-02 00:00:31</td>\n",
              "      <td>14.63</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-05-02 00:01:31</td>\n",
              "      <td>14.76</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-05-02 00:02:31</td>\n",
              "      <td>13.64</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014-05-02 00:03:31</td>\n",
              "      <td>14.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-05-02 00:04:31</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2014-05-02 00:05:31</td>\n",
              "      <td>14.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2014-05-02 00:06:31</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2014-05-02 00:07:31</td>\n",
              "      <td>14.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2014-05-02 00:08:31</td>\n",
              "      <td>14.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2014-05-02 00:09:31</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2014-05-02 00:10:31</td>\n",
              "      <td>14.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "      <td>2.87</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2014-05-02 00:11:31</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2014-05-02 00:12:31</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2014-05-02 00:13:31</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2014-05-02 00:15:31</td>\n",
              "      <td>14.69</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15 rows × 51 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a54b2b-88ff-4478-bc4e-cf423b8ac7fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66a54b2b-88ff-4478-bc4e-cf423b8ac7fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66a54b2b-88ff-4478-bc4e-cf423b8ac7fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "MUqjnm8Ur3bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![2](https://raw.githubusercontent.com/Lamp04ka/1/main/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202023-05-28%20215611.png)"
      ],
      "metadata": {
        "id": "j5P1RY1Lvwec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Lamp04ka/1/blob/main/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202023-05-28%20215611.png"
      ],
      "metadata": {
        "id": "XY5VHAOvvO2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.reindex(np.linspace(109,124,16,dtype=int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "iHRbXAY3r3xQ",
        "outputId": "61655718-3481-4538-f3b4-9a89084f0eda"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   time  Ubs,V  Ibs,A  Isun,A  Ipt1,A  Ipt2,A  Ipt3,A  Ipt4,A  \\\n",
              "109 2014-05-02 01:49:34  14.89   0.24    0.99    0.09    0.07     0.0     0.0   \n",
              "110 2014-05-02 01:50:34  14.83   0.19    0.27    0.09    0.07     0.0     0.0   \n",
              "111 2014-05-02 01:51:34  14.83   0.21    0.48    0.09    0.09     0.0     0.0   \n",
              "112 2014-05-02 01:52:34  14.89   0.24    0.88    0.09    0.09     0.0     0.0   \n",
              "113 2014-05-02 01:53:34  14.76   0.19    0.13    0.09    0.07     0.0     0.0   \n",
              "114 2014-05-02 01:54:34  14.83   0.21    0.69    0.09    0.07     0.0     0.0   \n",
              "115 2014-05-02 01:55:34  14.83   0.21    0.64    0.07    0.07     0.0     0.0   \n",
              "116 2014-05-02 01:56:34  14.83   0.24    0.59    0.09    0.07     0.0     0.0   \n",
              "117 2014-05-02 01:57:34  14.83   0.21    0.80    0.07    0.09     0.0     0.0   \n",
              "118 2014-05-02 01:58:34    NaN    NaN     NaN     NaN     NaN     NaN     NaN   \n",
              "119 2014-05-02 01:59:34  14.63   0.24    0.00    0.07    0.11     0.0     0.0   \n",
              "120 2014-05-02 02:00:34  14.63   0.21    0.00    0.11    0.07     0.0     0.0   \n",
              "121 2014-05-02 02:01:34  14.56   0.21    0.00    0.09    0.09     0.0     0.0   \n",
              "122 2014-05-02 02:02:34  14.76   0.24    0.99    0.07    0.07     0.0     0.0   \n",
              "123 2014-05-02 02:03:34  14.89   0.21    1.31    0.07    0.07     0.0     0.0   \n",
              "124 2014-05-02 02:04:34  14.83   0.24    0.83    0.09    0.09     0.0     0.0   \n",
              "\n",
              "     Ipt5,A  Ipt6,A  ...  TDS7,C  TDS8,C  TDS9,C  TKpt,C  TGbv,C  TNap,C  \\\n",
              "109     0.0     0.0  ...    17.0    16.0    16.0    24.0    16.0    16.0   \n",
              "110     0.0     0.0  ...    17.0    17.0    16.0    24.0    16.0    16.0   \n",
              "111     0.0     0.0  ...    17.0    17.0    16.0    24.0    16.0    16.0   \n",
              "112     0.0     0.0  ...    17.0    17.0    16.0    24.0    16.0    16.0   \n",
              "113     0.0     0.0  ...    17.0    17.0    17.0    24.0    16.0    16.0   \n",
              "114     0.0     0.0  ...    17.0    17.0    17.0    24.0    16.0    16.0   \n",
              "115     0.0     0.0  ...    17.0    17.0    17.0    24.0    31.0    16.0   \n",
              "116     0.0     0.0  ...    17.0    17.0    17.0    24.0    16.0    16.0   \n",
              "117     0.0     0.0  ...    17.0    17.0    17.0    24.0    17.0    16.0   \n",
              "118     NaN     NaN  ...    17.0    17.0    17.0    24.0    17.0    16.0   \n",
              "119     0.0     0.0  ...    17.0    17.0    17.0    24.0    17.0    16.0   \n",
              "120     0.0     0.0  ...    17.0    17.0    17.0    24.0    17.0    16.0   \n",
              "121     0.0     0.0  ...    18.0    17.0    17.0    24.0    17.0    16.0   \n",
              "122     0.0     0.0  ...    18.0    18.0    17.0    24.0    17.0    16.0   \n",
              "123     0.0     0.0  ...    18.0    18.0    17.0    24.0    17.0    16.0   \n",
              "124     0.0     0.0  ...    18.0    18.0    17.0    24.0    17.0    16.0   \n",
              "\n",
              "     TPrd2,C  TPrd1,C  TDS24,C  Class  \n",
              "109     17.0     17.0     20.0      0  \n",
              "110     17.0     17.0     20.0      1  \n",
              "111     17.0     17.0     74.0      1  \n",
              "112     17.0     17.0     20.0      1  \n",
              "113     17.0     17.0     20.0      1  \n",
              "114     17.0     17.0     20.0      0  \n",
              "115     18.0     17.0     20.0      0  \n",
              "116     18.0     17.0     20.0      0  \n",
              "117     18.0     17.0     20.0      0  \n",
              "118     18.0     17.0     20.0      2  \n",
              "119     18.0     18.0      NaN      0  \n",
              "120     18.0     18.0     20.0      0  \n",
              "121     18.0     18.0     20.0      0  \n",
              "122     18.0     18.0     20.0      0  \n",
              "123     18.0     18.0     20.0      0  \n",
              "124     18.0     18.0     20.0      0  \n",
              "\n",
              "[16 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42c19c29-ad3b-45b1-afe1-b823f8988cd4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>Ubs,V</th>\n",
              "      <th>Ibs,A</th>\n",
              "      <th>Isun,A</th>\n",
              "      <th>Ipt1,A</th>\n",
              "      <th>Ipt2,A</th>\n",
              "      <th>Ipt3,A</th>\n",
              "      <th>Ipt4,A</th>\n",
              "      <th>Ipt5,A</th>\n",
              "      <th>Ipt6,A</th>\n",
              "      <th>...</th>\n",
              "      <th>TDS7,C</th>\n",
              "      <th>TDS8,C</th>\n",
              "      <th>TDS9,C</th>\n",
              "      <th>TKpt,C</th>\n",
              "      <th>TGbv,C</th>\n",
              "      <th>TNap,C</th>\n",
              "      <th>TPrd2,C</th>\n",
              "      <th>TPrd1,C</th>\n",
              "      <th>TDS24,C</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>2014-05-02 01:49:34</td>\n",
              "      <td>14.89</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>2014-05-02 01:50:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>2014-05-02 01:51:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>2014-05-02 01:52:34</td>\n",
              "      <td>14.89</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>2014-05-02 01:53:34</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>2014-05-02 01:54:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>2014-05-02 01:55:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>2014-05-02 01:56:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>2014-05-02 01:57:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>2014-05-02 01:58:34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>2014-05-02 01:59:34</td>\n",
              "      <td>14.63</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>2014-05-02 02:00:34</td>\n",
              "      <td>14.63</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>2014-05-02 02:01:34</td>\n",
              "      <td>14.56</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>2014-05-02 02:02:34</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>2014-05-02 02:03:34</td>\n",
              "      <td>14.89</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>2014-05-02 02:04:34</td>\n",
              "      <td>14.83</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16 rows × 51 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42c19c29-ad3b-45b1-afe1-b823f8988cd4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42c19c29-ad3b-45b1-afe1-b823f8988cd4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42c19c29-ad3b-45b1-afe1-b823f8988cd4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как можно увидеть класы разбиты коректно"
      ],
      "metadata": {
        "id": "tSJwNtw7sqHS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC7Lftft1g8D"
      },
      "source": [
        "##Заполнение пробелов (NaN) в признаках"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qnRQodMSGrVk",
        "outputId": "992f97db-4548-4b9b-f86d-f878108ee9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1438 entries, 0 to 1437\n",
            "Data columns (total 51 columns):\n",
            " #   Column   Non-Null Count  Dtype         \n",
            "---  ------   --------------  -----         \n",
            " 0   time     1438 non-null   datetime64[ns]\n",
            " 1   Ubs,V    1438 non-null   float64       \n",
            " 2   Ibs,A    1438 non-null   float64       \n",
            " 3   Isun,A   1438 non-null   float64       \n",
            " 4   Ipt1,A   1438 non-null   float64       \n",
            " 5   Ipt2,A   1438 non-null   float64       \n",
            " 6   Ipt3,A   1438 non-null   float64       \n",
            " 7   Ipt4,A   1438 non-null   float64       \n",
            " 8   Ipt5,A   1438 non-null   float64       \n",
            " 9   Ipt6,A   1438 non-null   float64       \n",
            " 10  Ipt7,A   1438 non-null   float64       \n",
            " 11  Ipt10,A  1438 non-null   float64       \n",
            " 12  Ipt11,A  1438 non-null   float64       \n",
            " 13  Ipt12,A  1438 non-null   float64       \n",
            " 14  Ipt13,A  1438 non-null   float64       \n",
            " 15  Ipt14,A  1438 non-null   float64       \n",
            " 16  Ipt15,A  1438 non-null   float64       \n",
            " 17  Ipt16,A  1438 non-null   float64       \n",
            " 18  Ipt17,A  1438 non-null   float64       \n",
            " 19  TR1,C    1438 non-null   float64       \n",
            " 20  TR2,C    1438 non-null   float64       \n",
            " 21  TR3,C    1438 non-null   float64       \n",
            " 22  TR4,C    1438 non-null   float64       \n",
            " 23  TR5,C    1438 non-null   float64       \n",
            " 24  TR6,C    1438 non-null   float64       \n",
            " 25  TR7,C    1438 non-null   float64       \n",
            " 26  TR8,C    1438 non-null   float64       \n",
            " 27  TR9,C    1438 non-null   float64       \n",
            " 28  TR10,C   1438 non-null   float64       \n",
            " 29  TR11,C   1438 non-null   float64       \n",
            " 30  TR12,C   1438 non-null   float64       \n",
            " 31  TR13,C   1438 non-null   float64       \n",
            " 32  TR14,C   1438 non-null   float64       \n",
            " 33  TR15,C   1438 non-null   float64       \n",
            " 34  TR16,C   1438 non-null   float64       \n",
            " 35  TDS1,C   1438 non-null   float64       \n",
            " 36  TDS2,C   1438 non-null   float64       \n",
            " 37  TDS3,C   1438 non-null   float64       \n",
            " 38  TDS4,C   1438 non-null   float64       \n",
            " 39  TDS5,C   1438 non-null   float64       \n",
            " 40  TDS6,C   1438 non-null   float64       \n",
            " 41  TDS7,C   1438 non-null   float64       \n",
            " 42  TDS8,C   1438 non-null   float64       \n",
            " 43  TDS9,C   1438 non-null   float64       \n",
            " 44  TKpt,C   1438 non-null   float64       \n",
            " 45  TGbv,C   1438 non-null   float64       \n",
            " 46  TNap,C   1438 non-null   float64       \n",
            " 47  TPrd2,C  1438 non-null   float64       \n",
            " 48  TPrd1,C  1438 non-null   float64       \n",
            " 49  TDS24,C  1438 non-null   float64       \n",
            " 50  Class    1438 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(49), int64(1)\n",
            "memory usage: 573.1 KB\n"
          ]
        },
        {
          "data": {
            "text/plain": "                 time  Ubs,V  Ibs,A  Isun,A  Ipt1,A  Ipt2,A  Ipt3,A  Ipt4,A  \\\n0 2014-05-02 00:00:31  14.63   0.27    0.13    0.09    0.09     0.0     0.0   \n1 2014-05-02 00:01:31  14.76   0.27    0.13    0.09    0.07     0.0     0.0   \n2 2014-05-02 00:02:31  13.64   0.24    0.72    0.09    0.09     0.0     0.0   \n3 2014-05-02 00:03:31  14.69   0.21    0.56    0.07    0.09     0.0     0.0   \n4 2014-05-02 00:04:31  14.76   0.21    0.83    0.07    0.07     0.0     0.0   \n\n   Ipt5,A  Ipt6,A  ...  TDS7,C  TDS8,C  TDS9,C  TKpt,C  TGbv,C  TNap,C  \\\n0     0.0     0.0  ...    15.0    15.0    15.0    24.0    16.0    16.0   \n1     0.0     0.0  ...    15.0    15.0    15.0    24.0    16.0    16.0   \n2     0.0     0.0  ...    16.0    15.0    15.0    24.0    16.0    16.0   \n3     0.0     0.0  ...    16.0    16.0    15.0    24.0    16.0    16.0   \n4     0.0     0.0  ...    16.0    16.0    15.0    24.0    16.0    16.0   \n\n   TPrd2,C  TPrd1,C  TDS24,C  Class  \n0     16.0     16.0     19.0      0  \n1     16.0     16.0     19.0      0  \n2     16.0     16.0     19.0      0  \n3     16.0     16.0     19.0      0  \n4     17.0     16.0     19.0      0  \n\n[5 rows x 51 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>Ubs,V</th>\n      <th>Ibs,A</th>\n      <th>Isun,A</th>\n      <th>Ipt1,A</th>\n      <th>Ipt2,A</th>\n      <th>Ipt3,A</th>\n      <th>Ipt4,A</th>\n      <th>Ipt5,A</th>\n      <th>Ipt6,A</th>\n      <th>...</th>\n      <th>TDS7,C</th>\n      <th>TDS8,C</th>\n      <th>TDS9,C</th>\n      <th>TKpt,C</th>\n      <th>TGbv,C</th>\n      <th>TNap,C</th>\n      <th>TPrd2,C</th>\n      <th>TPrd1,C</th>\n      <th>TDS24,C</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2014-05-02 00:00:31</td>\n      <td>14.63</td>\n      <td>0.27</td>\n      <td>0.13</td>\n      <td>0.09</td>\n      <td>0.09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>24.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2014-05-02 00:01:31</td>\n      <td>14.76</td>\n      <td>0.27</td>\n      <td>0.13</td>\n      <td>0.09</td>\n      <td>0.07</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>24.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2014-05-02 00:02:31</td>\n      <td>13.64</td>\n      <td>0.24</td>\n      <td>0.72</td>\n      <td>0.09</td>\n      <td>0.09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>24.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2014-05-02 00:03:31</td>\n      <td>14.69</td>\n      <td>0.21</td>\n      <td>0.56</td>\n      <td>0.07</td>\n      <td>0.09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>24.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2014-05-02 00:04:31</td>\n      <td>14.76</td>\n      <td>0.21</td>\n      <td>0.83</td>\n      <td>0.07</td>\n      <td>0.07</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>24.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 51 columns</p>\n</div>"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data.ffill()\n",
        "data.info()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBIcxZuHSJCd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "data_dig = data.drop([\"time\"],axis=1)\n",
        "data_1 = data.drop([\"time\"],axis=1)\n",
        "data_2 = pd.DataFrame(scaler.fit_transform(data_dig),  columns = data_dig.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCDX_7vFbca_",
        "outputId": "73d66074-f089-4bb1-8744-aa5935172f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GBRT data 1 r2:  0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "X1 = data_1.drop([\"Class\"],axis=1)\n",
        "y = data_1[\"Class\"]\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, random_state=0)\n",
        "\n",
        "gbrt = GradientBoostingClassifier()\n",
        "gbrt.fit(X1_train, y1_train)\n",
        "print(\"GBRT data 1 r2: \", gbrt.score(X1_test,y1_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO3nCwdagRU7",
        "outputId": "b8e88e35-a3d5-431b-a96a-b5838d1fb156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GBRT2 data 2:  0.9305555555555556\n"
          ]
        }
      ],
      "source": [
        "X2 = data_2.drop([\"Class\"],axis=1)\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, random_state=0)\n",
        "\n",
        "gbrt2 = GradientBoostingClassifier()\n",
        "gbrt2.fit(X2_train, y2_train)\n",
        "y2_pred = gbrt2.predict(X2_test)\n",
        "print(\"GBRT2 data 2: \", gbrt2.score(X2_test,y2_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "ahocXQQgi2HO",
        "outputId": "a94e8566-2988-4f39-882d-16e08183035f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 49, 128)           640       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 24, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 24, 128)           65664     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,691\n",
            "Trainable params: 66,691\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 57ms/step - loss: 4.9817 - acc: 0.6929 - val_loss: 2.9911 - val_acc: 0.8806\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2.3623 - acc: 0.8905 - val_loss: 1.6012 - val_acc: 0.8806\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1.4173 - acc: 0.9026 - val_loss: 0.9390 - val_acc: 0.9167\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.1398 - acc: 0.9119 - val_loss: 0.5318 - val_acc: 0.9194\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.5852 - acc: 0.9295 - val_loss: 0.5151 - val_acc: 0.9139\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3284 - acc: 0.9369 - val_loss: 0.3349 - val_acc: 0.9194\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2062 - acc: 0.9453 - val_loss: 0.2518 - val_acc: 0.9222\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1592 - acc: 0.9536 - val_loss: 0.2854 - val_acc: 0.9306\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1086 - acc: 0.9694 - val_loss: 0.2527 - val_acc: 0.9194\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1009 - acc: 0.9694 - val_loss: 0.2313 - val_acc: 0.9250\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0971 - acc: 0.9740 - val_loss: 0.2265 - val_acc: 0.9222\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1065 - acc: 0.9675 - val_loss: 0.1878 - val_acc: 0.9333\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1229 - acc: 0.9722 - val_loss: 0.2229 - val_acc: 0.9417\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0756 - acc: 0.9805 - val_loss: 0.2319 - val_acc: 0.9361\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0625 - acc: 0.9805 - val_loss: 0.2186 - val_acc: 0.9306\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0626 - acc: 0.9824 - val_loss: 0.1994 - val_acc: 0.9417\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0631 - acc: 0.9861 - val_loss: 0.2897 - val_acc: 0.9278\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0572 - acc: 0.9852 - val_loss: 0.2836 - val_acc: 0.9278\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0750 - acc: 0.9759 - val_loss: 0.1796 - val_acc: 0.9417\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0591 - acc: 0.9805 - val_loss: 0.2115 - val_acc: 0.9361\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0738 - acc: 0.9768 - val_loss: 0.2354 - val_acc: 0.9361\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0505 - acc: 0.9842 - val_loss: 0.2397 - val_acc: 0.9333\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0482 - acc: 0.9879 - val_loss: 0.2692 - val_acc: 0.9306\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0507 - acc: 0.9861 - val_loss: 0.2436 - val_acc: 0.9278\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0515 - acc: 0.9842 - val_loss: 0.2684 - val_acc: 0.9250\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1102 - acc: 0.9740 - val_loss: 0.2388 - val_acc: 0.9361\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0950 - acc: 0.9731 - val_loss: 0.2493 - val_acc: 0.9333\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1323 - acc: 0.9610 - val_loss: 0.2897 - val_acc: 0.9306\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1593 - acc: 0.9601 - val_loss: 0.4455 - val_acc: 0.9000\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0793 - acc: 0.9805 - val_loss: 0.2675 - val_acc: 0.9250\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0599 - acc: 0.9870 - val_loss: 0.1826 - val_acc: 0.9444\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0522 - acc: 0.9879 - val_loss: 0.2788 - val_acc: 0.9278\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0591 - acc: 0.9796 - val_loss: 0.2673 - val_acc: 0.9333\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0429 - acc: 0.9852 - val_loss: 0.1957 - val_acc: 0.9500\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1115 - acc: 0.9694 - val_loss: 0.2350 - val_acc: 0.9389\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0744 - acc: 0.9796 - val_loss: 0.3380 - val_acc: 0.9361\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0501 - acc: 0.9852 - val_loss: 0.2654 - val_acc: 0.9278\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0692 - acc: 0.9879 - val_loss: 0.3078 - val_acc: 0.9417\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0476 - acc: 0.9889 - val_loss: 0.2653 - val_acc: 0.9333\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0347 - acc: 0.9917 - val_loss: 0.2580 - val_acc: 0.9389\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0336 - acc: 0.9917 - val_loss: 0.2274 - val_acc: 0.9361\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0542 - acc: 0.9870 - val_loss: 0.2971 - val_acc: 0.9278\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1294 - acc: 0.9657 - val_loss: 0.2442 - val_acc: 0.9444\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1049 - acc: 0.9750 - val_loss: 0.2803 - val_acc: 0.9444\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1131 - acc: 0.9759 - val_loss: 0.3633 - val_acc: 0.9306\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0884 - acc: 0.9787 - val_loss: 0.2945 - val_acc: 0.9278\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0695 - acc: 0.9805 - val_loss: 0.2772 - val_acc: 0.9389\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0715 - acc: 0.9861 - val_loss: 0.2876 - val_acc: 0.9278\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0707 - acc: 0.9824 - val_loss: 0.4539 - val_acc: 0.9167\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0604 - acc: 0.9833 - val_loss: 0.2398 - val_acc: 0.9389\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0743 - acc: 0.9814 - val_loss: 0.3123 - val_acc: 0.9278\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0839 - acc: 0.9796 - val_loss: 0.3856 - val_acc: 0.9444\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0653 - acc: 0.9852 - val_loss: 0.3558 - val_acc: 0.9250\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0635 - acc: 0.9842 - val_loss: 0.3665 - val_acc: 0.9167\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0861 - acc: 0.9740 - val_loss: 0.2526 - val_acc: 0.9361\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0573 - acc: 0.9852 - val_loss: 0.2442 - val_acc: 0.9361\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0401 - acc: 0.9861 - val_loss: 0.2500 - val_acc: 0.9389\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0909 - acc: 0.9814 - val_loss: 0.2475 - val_acc: 0.9444\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0914 - acc: 0.9768 - val_loss: 0.2277 - val_acc: 0.9417\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0467 - acc: 0.9833 - val_loss: 0.3566 - val_acc: 0.9333\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0464 - acc: 0.9879 - val_loss: 0.2732 - val_acc: 0.9306\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0430 - acc: 0.9898 - val_loss: 0.5385 - val_acc: 0.9056\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0613 - acc: 0.9796 - val_loss: 0.2549 - val_acc: 0.9389\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0679 - acc: 0.9759 - val_loss: 0.2642 - val_acc: 0.9389\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0767 - acc: 0.9861 - val_loss: 0.3097 - val_acc: 0.9417\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0562 - acc: 0.9889 - val_loss: 0.2738 - val_acc: 0.9389\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0450 - acc: 0.9898 - val_loss: 0.2721 - val_acc: 0.9389\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0330 - acc: 0.9917 - val_loss: 0.2804 - val_acc: 0.9361\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0476 - acc: 0.9833 - val_loss: 0.3628 - val_acc: 0.9250\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0424 - acc: 0.9852 - val_loss: 0.2488 - val_acc: 0.9361\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0340 - acc: 0.9917 - val_loss: 0.2660 - val_acc: 0.9389\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0446 - acc: 0.9926 - val_loss: 0.2644 - val_acc: 0.9361\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0395 - acc: 0.9926 - val_loss: 0.2665 - val_acc: 0.9417\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0276 - acc: 0.9898 - val_loss: 0.3005 - val_acc: 0.9417\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0706 - acc: 0.9833 - val_loss: 0.2685 - val_acc: 0.9361\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1027 - acc: 0.9750 - val_loss: 0.4324 - val_acc: 0.9250\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0747 - acc: 0.9842 - val_loss: 0.2952 - val_acc: 0.9389\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0560 - acc: 0.9833 - val_loss: 0.2857 - val_acc: 0.9333\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0280 - acc: 0.9935 - val_loss: 0.2913 - val_acc: 0.9500\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0348 - acc: 0.9879 - val_loss: 0.2388 - val_acc: 0.9444\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0299 - acc: 0.9935 - val_loss: 0.3104 - val_acc: 0.9333\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0492 - acc: 0.9898 - val_loss: 0.2407 - val_acc: 0.9472\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0830 - acc: 0.9777 - val_loss: 0.2298 - val_acc: 0.9500\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0541 - acc: 0.9879 - val_loss: 0.3159 - val_acc: 0.9306\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0293 - acc: 0.9917 - val_loss: 0.2738 - val_acc: 0.9417\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0570 - acc: 0.9898 - val_loss: 0.3979 - val_acc: 0.9278\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0409 - acc: 0.9907 - val_loss: 0.2612 - val_acc: 0.9389\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0271 - acc: 0.9917 - val_loss: 0.3870 - val_acc: 0.9222\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0372 - acc: 0.9898 - val_loss: 0.2772 - val_acc: 0.9417\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0530 - acc: 0.9870 - val_loss: 0.2632 - val_acc: 0.9417\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0633 - acc: 0.9889 - val_loss: 0.4844 - val_acc: 0.9056\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0634 - acc: 0.9879 - val_loss: 0.5176 - val_acc: 0.9194\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0657 - acc: 0.9768 - val_loss: 0.3734 - val_acc: 0.9417\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0695 - acc: 0.9842 - val_loss: 0.3309 - val_acc: 0.9389\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0769 - acc: 0.9805 - val_loss: 0.5503 - val_acc: 0.9194\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0795 - acc: 0.9796 - val_loss: 0.3182 - val_acc: 0.9306\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0230 - acc: 0.9917 - val_loss: 0.3344 - val_acc: 0.9306\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0803 - acc: 0.9777 - val_loss: 0.3128 - val_acc: 0.9250\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0619 - acc: 0.9824 - val_loss: 0.4408 - val_acc: 0.9111\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0747 - acc: 0.9842 - val_loss: 0.2938 - val_acc: 0.9361\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "# Создание модели\n",
        "model = Sequential()\n",
        "y_train = tf.keras.utils.to_categorical(y1_train, 3)\n",
        "y_test = tf.keras.utils.to_categorical(y1_test, 3)\n",
        "model.add(layers.Conv1D(128, 4, activation='relu', padding='same', input_shape=(X1.shape[1], 1)))\n",
        "model.add(layers.MaxPooling1D(2))\n",
        "model.add(layers.Conv1D(128, 4, activation='relu', padding='same'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(3, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Обучение модели\n",
        "m=model.fit(X1_train, y_train, validation_data=(X1_test, y_test), epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.949999988079071\n"
          ]
        }
      ],
      "source": [
        "print(np.max(m.history[\"val_acc\"]))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GLuIIh7LnUyR",
        "outputId": "6c762f8d-018e-4190-a036-3a8d560eb138"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_44 (Conv1D)          (None, 49, 128)           640       \n",
            "                                                                 \n",
            " global_max_pooling1d_12 (Gl  (None, 128)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,027\n",
            "Trainable params: 1,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 2s 44ms/step - loss: 2.1609 - acc: 0.6957 - val_loss: 1.1469 - val_acc: 0.7833\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8149 - acc: 0.8627 - val_loss: 0.6024 - val_acc: 0.8806\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6683 - acc: 0.8896 - val_loss: 0.5679 - val_acc: 0.8861\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.5549 - acc: 0.9026 - val_loss: 0.3882 - val_acc: 0.8778\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4273 - acc: 0.9072 - val_loss: 0.4200 - val_acc: 0.8889\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3933 - acc: 0.9035 - val_loss: 0.4187 - val_acc: 0.8917\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3815 - acc: 0.9063 - val_loss: 0.3464 - val_acc: 0.8889\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3713 - acc: 0.9147 - val_loss: 0.4353 - val_acc: 0.8833\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3420 - acc: 0.9156 - val_loss: 0.3338 - val_acc: 0.8972\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3144 - acc: 0.9239 - val_loss: 0.3538 - val_acc: 0.8972\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2821 - acc: 0.9332 - val_loss: 0.3148 - val_acc: 0.8944\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2905 - acc: 0.9286 - val_loss: 0.3124 - val_acc: 0.8917\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2563 - acc: 0.9369 - val_loss: 0.3204 - val_acc: 0.9083\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2759 - acc: 0.9239 - val_loss: 0.3085 - val_acc: 0.8889\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2541 - acc: 0.9332 - val_loss: 0.2792 - val_acc: 0.9139\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2339 - acc: 0.9286 - val_loss: 0.2775 - val_acc: 0.9000\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2276 - acc: 0.9323 - val_loss: 0.3309 - val_acc: 0.8917\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2485 - acc: 0.9276 - val_loss: 0.2626 - val_acc: 0.9139\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2061 - acc: 0.9416 - val_loss: 0.2803 - val_acc: 0.8944\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2110 - acc: 0.9369 - val_loss: 0.2955 - val_acc: 0.8944\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1974 - acc: 0.9351 - val_loss: 0.2795 - val_acc: 0.9028\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1964 - acc: 0.9406 - val_loss: 0.2560 - val_acc: 0.9167\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1864 - acc: 0.9434 - val_loss: 0.2821 - val_acc: 0.9056\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1861 - acc: 0.9443 - val_loss: 0.2740 - val_acc: 0.9000\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1829 - acc: 0.9397 - val_loss: 0.2828 - val_acc: 0.9056\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1924 - acc: 0.9369 - val_loss: 0.4232 - val_acc: 0.8611\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2027 - acc: 0.9416 - val_loss: 0.2503 - val_acc: 0.9306\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2134 - acc: 0.9388 - val_loss: 0.2462 - val_acc: 0.9167\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1844 - acc: 0.9434 - val_loss: 0.2878 - val_acc: 0.9028\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1877 - acc: 0.9416 - val_loss: 0.2946 - val_acc: 0.8972\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1787 - acc: 0.9462 - val_loss: 0.2615 - val_acc: 0.8972\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1669 - acc: 0.9490 - val_loss: 0.2681 - val_acc: 0.9028\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1710 - acc: 0.9453 - val_loss: 0.2653 - val_acc: 0.9028\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1725 - acc: 0.9443 - val_loss: 0.2696 - val_acc: 0.9028\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1656 - acc: 0.9434 - val_loss: 0.2590 - val_acc: 0.9000\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1658 - acc: 0.9490 - val_loss: 0.3205 - val_acc: 0.8917\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1696 - acc: 0.9471 - val_loss: 0.2632 - val_acc: 0.9028\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1571 - acc: 0.9481 - val_loss: 0.2951 - val_acc: 0.9194\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1825 - acc: 0.9388 - val_loss: 0.3482 - val_acc: 0.9028\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1871 - acc: 0.9406 - val_loss: 0.3124 - val_acc: 0.9222\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1583 - acc: 0.9481 - val_loss: 0.2549 - val_acc: 0.9111\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1481 - acc: 0.9499 - val_loss: 0.2620 - val_acc: 0.9028\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1517 - acc: 0.9518 - val_loss: 0.2906 - val_acc: 0.8944\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1562 - acc: 0.9527 - val_loss: 0.3006 - val_acc: 0.8917\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1667 - acc: 0.9443 - val_loss: 0.2498 - val_acc: 0.9000\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1400 - acc: 0.9564 - val_loss: 0.2917 - val_acc: 0.9028\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1396 - acc: 0.9545 - val_loss: 0.2526 - val_acc: 0.9000\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1398 - acc: 0.9545 - val_loss: 0.2560 - val_acc: 0.9000\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1297 - acc: 0.9610 - val_loss: 0.2480 - val_acc: 0.9111\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1397 - acc: 0.9583 - val_loss: 0.2576 - val_acc: 0.9083\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1284 - acc: 0.9583 - val_loss: 0.2515 - val_acc: 0.9028\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1347 - acc: 0.9573 - val_loss: 0.2452 - val_acc: 0.9056\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1353 - acc: 0.9592 - val_loss: 0.2509 - val_acc: 0.9000\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1473 - acc: 0.9490 - val_loss: 0.3104 - val_acc: 0.9028\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1421 - acc: 0.9490 - val_loss: 0.2514 - val_acc: 0.9028\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1336 - acc: 0.9564 - val_loss: 0.2491 - val_acc: 0.9083\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1451 - acc: 0.9573 - val_loss: 0.2827 - val_acc: 0.9056\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1262 - acc: 0.9573 - val_loss: 0.2663 - val_acc: 0.9000\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1296 - acc: 0.9573 - val_loss: 0.2450 - val_acc: 0.9028\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1307 - acc: 0.9573 - val_loss: 0.2454 - val_acc: 0.9056\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1387 - acc: 0.9518 - val_loss: 0.2786 - val_acc: 0.9056\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1674 - acc: 0.9471 - val_loss: 0.2679 - val_acc: 0.9056\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1450 - acc: 0.9555 - val_loss: 0.2558 - val_acc: 0.8972\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1293 - acc: 0.9573 - val_loss: 0.2762 - val_acc: 0.9000\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1263 - acc: 0.9583 - val_loss: 0.2492 - val_acc: 0.9028\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1219 - acc: 0.9620 - val_loss: 0.2400 - val_acc: 0.9111\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1286 - acc: 0.9592 - val_loss: 0.2521 - val_acc: 0.9056\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1171 - acc: 0.9610 - val_loss: 0.2594 - val_acc: 0.9083\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1248 - acc: 0.9564 - val_loss: 0.2789 - val_acc: 0.8972\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1186 - acc: 0.9638 - val_loss: 0.2528 - val_acc: 0.9028\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1236 - acc: 0.9536 - val_loss: 0.3025 - val_acc: 0.9000\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1293 - acc: 0.9583 - val_loss: 0.2383 - val_acc: 0.9083\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1268 - acc: 0.9555 - val_loss: 0.2439 - val_acc: 0.9194\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1206 - acc: 0.9601 - val_loss: 0.2521 - val_acc: 0.8972\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1130 - acc: 0.9638 - val_loss: 0.2577 - val_acc: 0.9028\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1101 - acc: 0.9620 - val_loss: 0.2403 - val_acc: 0.9056\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1133 - acc: 0.9629 - val_loss: 0.2401 - val_acc: 0.9139\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1197 - acc: 0.9592 - val_loss: 0.2328 - val_acc: 0.9056\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1180 - acc: 0.9583 - val_loss: 0.2537 - val_acc: 0.9056\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1180 - acc: 0.9620 - val_loss: 0.2452 - val_acc: 0.9167\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1149 - acc: 0.9620 - val_loss: 0.2381 - val_acc: 0.9111\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1142 - acc: 0.9610 - val_loss: 0.2504 - val_acc: 0.9167\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1169 - acc: 0.9610 - val_loss: 0.2427 - val_acc: 0.9111\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1162 - acc: 0.9610 - val_loss: 0.2524 - val_acc: 0.9083\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1166 - acc: 0.9647 - val_loss: 0.2736 - val_acc: 0.9028\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1415 - acc: 0.9462 - val_loss: 0.2492 - val_acc: 0.9056\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1077 - acc: 0.9657 - val_loss: 0.2285 - val_acc: 0.9083\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1068 - acc: 0.9629 - val_loss: 0.2369 - val_acc: 0.9194\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1104 - acc: 0.9629 - val_loss: 0.2421 - val_acc: 0.9028\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1025 - acc: 0.9657 - val_loss: 0.2402 - val_acc: 0.9139\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1152 - acc: 0.9592 - val_loss: 0.2573 - val_acc: 0.9167\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1469 - acc: 0.9462 - val_loss: 0.3108 - val_acc: 0.9083\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1403 - acc: 0.9508 - val_loss: 0.2515 - val_acc: 0.9222\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1225 - acc: 0.9555 - val_loss: 0.2346 - val_acc: 0.9167\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1251 - acc: 0.9592 - val_loss: 0.2215 - val_acc: 0.9111\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1084 - acc: 0.9647 - val_loss: 0.2248 - val_acc: 0.9222\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1120 - acc: 0.9629 - val_loss: 0.2429 - val_acc: 0.9111\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1174 - acc: 0.9638 - val_loss: 0.2880 - val_acc: 0.9139\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1347 - acc: 0.9536 - val_loss: 0.2312 - val_acc: 0.9139\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1283 - acc: 0.9592 - val_loss: 0.2172 - val_acc: 0.9250\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_45 (Conv1D)          (None, 49, 128)           640       \n",
            "                                                                 \n",
            " max_pooling1d_32 (MaxPoolin  (None, 24, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_46 (Conv1D)          (None, 24, 128)           65664     \n",
            "                                                                 \n",
            " global_max_pooling1d_13 (Gl  (None, 128)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,691\n",
            "Trainable params: 66,691\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 42ms/step - loss: 2.2486 - acc: 0.8692 - val_loss: 0.7440 - val_acc: 0.9167\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9096 - acc: 0.9202 - val_loss: 0.3608 - val_acc: 0.9333\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.4177 - acc: 0.9276 - val_loss: 0.4967 - val_acc: 0.9083\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2544 - acc: 0.9323 - val_loss: 0.3453 - val_acc: 0.9194\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1998 - acc: 0.9499 - val_loss: 0.2415 - val_acc: 0.9417\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1895 - acc: 0.9471 - val_loss: 0.3673 - val_acc: 0.9167\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2002 - acc: 0.9462 - val_loss: 0.8342 - val_acc: 0.8778\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2732 - acc: 0.9295 - val_loss: 0.4584 - val_acc: 0.9222\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1548 - acc: 0.9666 - val_loss: 0.3485 - val_acc: 0.9167\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1164 - acc: 0.9740 - val_loss: 0.3581 - val_acc: 0.9306\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0959 - acc: 0.9768 - val_loss: 0.3217 - val_acc: 0.9222\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0760 - acc: 0.9750 - val_loss: 0.3275 - val_acc: 0.9306\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0627 - acc: 0.9814 - val_loss: 0.3659 - val_acc: 0.9194\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0746 - acc: 0.9824 - val_loss: 0.2985 - val_acc: 0.9278\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0583 - acc: 0.9852 - val_loss: 0.3394 - val_acc: 0.9111\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0684 - acc: 0.9824 - val_loss: 0.3191 - val_acc: 0.9278\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0910 - acc: 0.9759 - val_loss: 0.3477 - val_acc: 0.9167\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0873 - acc: 0.9796 - val_loss: 0.3416 - val_acc: 0.9278\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0762 - acc: 0.9870 - val_loss: 0.3662 - val_acc: 0.9278\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0586 - acc: 0.9861 - val_loss: 0.4045 - val_acc: 0.9167\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0543 - acc: 0.9833 - val_loss: 0.4072 - val_acc: 0.9167\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0828 - acc: 0.9750 - val_loss: 0.3369 - val_acc: 0.9083\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0625 - acc: 0.9759 - val_loss: 0.4687 - val_acc: 0.9028\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1635 - acc: 0.9629 - val_loss: 0.3026 - val_acc: 0.9278\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2462 - acc: 0.9490 - val_loss: 0.4446 - val_acc: 0.9194\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0893 - acc: 0.9796 - val_loss: 0.3681 - val_acc: 0.9222\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0543 - acc: 0.9852 - val_loss: 0.5198 - val_acc: 0.8972\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1036 - acc: 0.9740 - val_loss: 0.4480 - val_acc: 0.9278\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0851 - acc: 0.9796 - val_loss: 0.4119 - val_acc: 0.9111\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0674 - acc: 0.9824 - val_loss: 0.4576 - val_acc: 0.9194\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.0428 - acc: 0.9861 - val_loss: 0.4198 - val_acc: 0.9056\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.1242 - acc: 0.9675 - val_loss: 0.6493 - val_acc: 0.8889\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.1407 - acc: 0.9703 - val_loss: 0.3756 - val_acc: 0.9250\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1080 - acc: 0.9768 - val_loss: 0.5173 - val_acc: 0.9139\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0991 - acc: 0.9777 - val_loss: 0.4733 - val_acc: 0.9222\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0802 - acc: 0.9842 - val_loss: 0.3328 - val_acc: 0.9306\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.0905 - acc: 0.9805 - val_loss: 0.4004 - val_acc: 0.9333\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0449 - acc: 0.9889 - val_loss: 0.4611 - val_acc: 0.9194\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0946 - acc: 0.9787 - val_loss: 0.4097 - val_acc: 0.9139\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1291 - acc: 0.9768 - val_loss: 0.3747 - val_acc: 0.9333\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0719 - acc: 0.9824 - val_loss: 0.6478 - val_acc: 0.9167\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1022 - acc: 0.9805 - val_loss: 0.3333 - val_acc: 0.9389\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1008 - acc: 0.9750 - val_loss: 0.4413 - val_acc: 0.9306\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0940 - acc: 0.9740 - val_loss: 0.3622 - val_acc: 0.9222\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0658 - acc: 0.9870 - val_loss: 0.4458 - val_acc: 0.9361\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0846 - acc: 0.9824 - val_loss: 0.4179 - val_acc: 0.9167\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0578 - acc: 0.9833 - val_loss: 0.3791 - val_acc: 0.9361\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.0531 - acc: 0.9833 - val_loss: 0.5298 - val_acc: 0.9083\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0844 - acc: 0.9777 - val_loss: 0.6053 - val_acc: 0.9083\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0531 - acc: 0.9852 - val_loss: 0.3882 - val_acc: 0.9278\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.0728 - acc: 0.9805 - val_loss: 0.3848 - val_acc: 0.9472\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0826 - acc: 0.9824 - val_loss: 0.3833 - val_acc: 0.9333\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0557 - acc: 0.9879 - val_loss: 0.4940 - val_acc: 0.9278\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0343 - acc: 0.9879 - val_loss: 0.3548 - val_acc: 0.9306\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0516 - acc: 0.9926 - val_loss: 0.4894 - val_acc: 0.9222\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0457 - acc: 0.9889 - val_loss: 0.4384 - val_acc: 0.9306\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0468 - acc: 0.9870 - val_loss: 0.3677 - val_acc: 0.9389\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0779 - acc: 0.9842 - val_loss: 0.6695 - val_acc: 0.9083\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0725 - acc: 0.9814 - val_loss: 0.4123 - val_acc: 0.9194\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0792 - acc: 0.9879 - val_loss: 0.4148 - val_acc: 0.9333\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0865 - acc: 0.9852 - val_loss: 0.6340 - val_acc: 0.9139\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1130 - acc: 0.9694 - val_loss: 0.4097 - val_acc: 0.9361\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1033 - acc: 0.9750 - val_loss: 0.5195 - val_acc: 0.9417\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0461 - acc: 0.9870 - val_loss: 0.3757 - val_acc: 0.9306\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0491 - acc: 0.9861 - val_loss: 0.4803 - val_acc: 0.9444\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0350 - acc: 0.9917 - val_loss: 0.4679 - val_acc: 0.9333\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0627 - acc: 0.9907 - val_loss: 0.5424 - val_acc: 0.9278\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0618 - acc: 0.9852 - val_loss: 0.4688 - val_acc: 0.9361\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0420 - acc: 0.9861 - val_loss: 0.5132 - val_acc: 0.9111\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0314 - acc: 0.9917 - val_loss: 0.4200 - val_acc: 0.9389\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0349 - acc: 0.9935 - val_loss: 0.4543 - val_acc: 0.9389\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0311 - acc: 0.9898 - val_loss: 0.4844 - val_acc: 0.9278\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0437 - acc: 0.9944 - val_loss: 0.3833 - val_acc: 0.9333\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0412 - acc: 0.9898 - val_loss: 0.4164 - val_acc: 0.9389\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0316 - acc: 0.9935 - val_loss: 0.4920 - val_acc: 0.9250\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0636 - acc: 0.9852 - val_loss: 0.4321 - val_acc: 0.9222\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0657 - acc: 0.9870 - val_loss: 0.4015 - val_acc: 0.9333\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0525 - acc: 0.9870 - val_loss: 0.4226 - val_acc: 0.9250\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0426 - acc: 0.9917 - val_loss: 0.4999 - val_acc: 0.9306\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0387 - acc: 0.9898 - val_loss: 0.4144 - val_acc: 0.9250\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0600 - acc: 0.9870 - val_loss: 0.4652 - val_acc: 0.9361\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0303 - acc: 0.9935 - val_loss: 0.4302 - val_acc: 0.9306\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0344 - acc: 0.9907 - val_loss: 0.5235 - val_acc: 0.9139\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0550 - acc: 0.9907 - val_loss: 0.4228 - val_acc: 0.9389\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0478 - acc: 0.9898 - val_loss: 0.3851 - val_acc: 0.9361\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0613 - acc: 0.9898 - val_loss: 0.7275 - val_acc: 0.9111\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0449 - acc: 0.9898 - val_loss: 0.4035 - val_acc: 0.9278\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0616 - acc: 0.9870 - val_loss: 0.4659 - val_acc: 0.9361\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0463 - acc: 0.9861 - val_loss: 0.4505 - val_acc: 0.9361\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0333 - acc: 0.9935 - val_loss: 0.5234 - val_acc: 0.9194\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1051 - acc: 0.9833 - val_loss: 0.5804 - val_acc: 0.9056\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1028 - acc: 0.9759 - val_loss: 0.4229 - val_acc: 0.9389\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0749 - acc: 0.9787 - val_loss: 0.4746 - val_acc: 0.9306\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0489 - acc: 0.9917 - val_loss: 0.4540 - val_acc: 0.9278\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0493 - acc: 0.9935 - val_loss: 0.4159 - val_acc: 0.9333\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0311 - acc: 0.9935 - val_loss: 0.4906 - val_acc: 0.9278\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0331 - acc: 0.9917 - val_loss: 0.4184 - val_acc: 0.9361\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0375 - acc: 0.9907 - val_loss: 0.5609 - val_acc: 0.9194\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0629 - acc: 0.9805 - val_loss: 0.5059 - val_acc: 0.9139\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0549 - acc: 0.9833 - val_loss: 0.4972 - val_acc: 0.9361\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_47 (Conv1D)          (None, 49, 128)           640       \n",
            "                                                                 \n",
            " max_pooling1d_33 (MaxPoolin  (None, 24, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_48 (Conv1D)          (None, 24, 128)           65664     \n",
            "                                                                 \n",
            " max_pooling1d_34 (MaxPoolin  (None, 12, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_49 (Conv1D)          (None, 12, 128)           65664     \n",
            "                                                                 \n",
            " global_max_pooling1d_14 (Gl  (None, 128)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132,355\n",
            "Trainable params: 132,355\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 2s 86ms/step - loss: 3.0612 - acc: 0.7941 - val_loss: 0.3908 - val_acc: 0.9278\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.6066 - acc: 0.9026 - val_loss: 0.3935 - val_acc: 0.9167\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3746 - acc: 0.9249 - val_loss: 0.1884 - val_acc: 0.9361\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2462 - acc: 0.9397 - val_loss: 0.2448 - val_acc: 0.9194\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1766 - acc: 0.9471 - val_loss: 0.1584 - val_acc: 0.9472\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.1341 - acc: 0.9601 - val_loss: 0.1691 - val_acc: 0.9472\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1208 - acc: 0.9620 - val_loss: 0.1400 - val_acc: 0.9500\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0821 - acc: 0.9722 - val_loss: 0.1541 - val_acc: 0.9389\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0735 - acc: 0.9796 - val_loss: 0.1572 - val_acc: 0.9389\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0768 - acc: 0.9768 - val_loss: 0.1463 - val_acc: 0.9528\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0601 - acc: 0.9814 - val_loss: 0.1382 - val_acc: 0.9472\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0541 - acc: 0.9852 - val_loss: 0.1222 - val_acc: 0.9583\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0533 - acc: 0.9852 - val_loss: 0.1356 - val_acc: 0.9583\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0432 - acc: 0.9889 - val_loss: 0.1423 - val_acc: 0.9556\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0470 - acc: 0.9861 - val_loss: 0.1392 - val_acc: 0.9528\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0694 - acc: 0.9852 - val_loss: 0.1766 - val_acc: 0.9500\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0502 - acc: 0.9917 - val_loss: 0.1698 - val_acc: 0.9417\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0433 - acc: 0.9870 - val_loss: 0.1296 - val_acc: 0.9611\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0363 - acc: 0.9907 - val_loss: 0.1586 - val_acc: 0.9556\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0414 - acc: 0.9879 - val_loss: 0.1407 - val_acc: 0.9528\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0376 - acc: 0.9879 - val_loss: 0.1614 - val_acc: 0.9472\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0576 - acc: 0.9889 - val_loss: 0.2232 - val_acc: 0.9361\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0456 - acc: 0.9907 - val_loss: 0.1584 - val_acc: 0.9528\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0307 - acc: 0.9917 - val_loss: 0.1686 - val_acc: 0.9528\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0318 - acc: 0.9898 - val_loss: 0.1444 - val_acc: 0.9528\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0360 - acc: 0.9889 - val_loss: 0.1441 - val_acc: 0.9611\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0367 - acc: 0.9907 - val_loss: 0.1959 - val_acc: 0.9417\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0304 - acc: 0.9898 - val_loss: 0.1343 - val_acc: 0.9583\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0485 - acc: 0.9842 - val_loss: 0.1506 - val_acc: 0.9556\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0439 - acc: 0.9926 - val_loss: 0.2657 - val_acc: 0.9333\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0354 - acc: 0.9879 - val_loss: 0.1617 - val_acc: 0.9500\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0344 - acc: 0.9898 - val_loss: 0.2152 - val_acc: 0.9333\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0292 - acc: 0.9917 - val_loss: 0.1647 - val_acc: 0.9611\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0298 - acc: 0.9926 - val_loss: 0.1824 - val_acc: 0.9528\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0306 - acc: 0.9917 - val_loss: 0.2006 - val_acc: 0.9472\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0271 - acc: 0.9926 - val_loss: 0.1526 - val_acc: 0.9528\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0214 - acc: 0.9926 - val_loss: 0.2541 - val_acc: 0.9333\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0492 - acc: 0.9870 - val_loss: 0.1827 - val_acc: 0.9472\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0502 - acc: 0.9889 - val_loss: 0.1788 - val_acc: 0.9583\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0794 - acc: 0.9833 - val_loss: 0.2829 - val_acc: 0.9306\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0849 - acc: 0.9833 - val_loss: 0.2759 - val_acc: 0.9250\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0779 - acc: 0.9768 - val_loss: 0.2652 - val_acc: 0.9389\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0765 - acc: 0.9787 - val_loss: 0.1602 - val_acc: 0.9583\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0469 - acc: 0.9842 - val_loss: 0.2333 - val_acc: 0.9389\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0462 - acc: 0.9879 - val_loss: 0.1703 - val_acc: 0.9500\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0217 - acc: 0.9935 - val_loss: 0.2057 - val_acc: 0.9389\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0278 - acc: 0.9917 - val_loss: 0.1742 - val_acc: 0.9500\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0294 - acc: 0.9935 - val_loss: 0.1856 - val_acc: 0.9528\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0229 - acc: 0.9935 - val_loss: 0.1940 - val_acc: 0.9472\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0338 - acc: 0.9926 - val_loss: 0.1631 - val_acc: 0.9528\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0443 - acc: 0.9907 - val_loss: 0.2635 - val_acc: 0.9333\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0347 - acc: 0.9870 - val_loss: 0.1469 - val_acc: 0.9583\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0232 - acc: 0.9917 - val_loss: 0.1990 - val_acc: 0.9472\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0204 - acc: 0.9926 - val_loss: 0.1607 - val_acc: 0.9528\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0349 - acc: 0.9917 - val_loss: 0.2080 - val_acc: 0.9417\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0255 - acc: 0.9907 - val_loss: 0.1931 - val_acc: 0.9472\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0180 - acc: 0.9944 - val_loss: 0.1770 - val_acc: 0.9583\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0230 - acc: 0.9926 - val_loss: 0.1961 - val_acc: 0.9472\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0274 - acc: 0.9907 - val_loss: 0.1861 - val_acc: 0.9556\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0216 - acc: 0.9944 - val_loss: 0.1788 - val_acc: 0.9583\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0178 - acc: 0.9935 - val_loss: 0.1802 - val_acc: 0.9583\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0277 - acc: 0.9898 - val_loss: 0.2159 - val_acc: 0.9417\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0268 - acc: 0.9898 - val_loss: 0.1816 - val_acc: 0.9528\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0205 - acc: 0.9926 - val_loss: 0.2377 - val_acc: 0.9361\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0284 - acc: 0.9907 - val_loss: 0.1786 - val_acc: 0.9611\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0245 - acc: 0.9917 - val_loss: 0.2284 - val_acc: 0.9444\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0312 - acc: 0.9926 - val_loss: 0.1698 - val_acc: 0.9528\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.2021 - val_acc: 0.9528\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0214 - acc: 0.9917 - val_loss: 0.1918 - val_acc: 0.9500\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0196 - acc: 0.9935 - val_loss: 0.1934 - val_acc: 0.9500\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0238 - acc: 0.9944 - val_loss: 0.1807 - val_acc: 0.9556\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.2270 - val_acc: 0.9444\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0162 - acc: 0.9935 - val_loss: 0.1661 - val_acc: 0.9611\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0210 - acc: 0.9926 - val_loss: 0.1818 - val_acc: 0.9556\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0170 - acc: 0.9944 - val_loss: 0.2010 - val_acc: 0.9528\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0159 - acc: 0.9926 - val_loss: 0.2001 - val_acc: 0.9528\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0161 - acc: 0.9944 - val_loss: 0.1885 - val_acc: 0.9556\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0123 - acc: 0.9917 - val_loss: 0.1952 - val_acc: 0.9500\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0166 - acc: 0.9944 - val_loss: 0.2088 - val_acc: 0.9528\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0355 - acc: 0.9935 - val_loss: 0.1680 - val_acc: 0.9528\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0243 - acc: 0.9935 - val_loss: 0.2417 - val_acc: 0.9444\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0234 - acc: 0.9935 - val_loss: 0.1683 - val_acc: 0.9556\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0280 - acc: 0.9935 - val_loss: 0.2530 - val_acc: 0.9444\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0239 - acc: 0.9944 - val_loss: 0.1877 - val_acc: 0.9583\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0298 - acc: 0.9944 - val_loss: 0.2008 - val_acc: 0.9528\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0221 - acc: 0.9935 - val_loss: 0.2484 - val_acc: 0.9500\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0188 - acc: 0.9926 - val_loss: 0.1751 - val_acc: 0.9556\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0138 - acc: 0.9935 - val_loss: 0.2101 - val_acc: 0.9528\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0184 - acc: 0.9935 - val_loss: 0.2044 - val_acc: 0.9528\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0159 - acc: 0.9935 - val_loss: 0.2251 - val_acc: 0.9472\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0158 - acc: 0.9935 - val_loss: 0.1876 - val_acc: 0.9556\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0168 - acc: 0.9954 - val_loss: 0.2522 - val_acc: 0.9389\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0155 - acc: 0.9954 - val_loss: 0.1909 - val_acc: 0.9500\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0175 - acc: 0.9917 - val_loss: 0.2170 - val_acc: 0.9556\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.1856 - val_acc: 0.9583\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0227 - acc: 0.9926 - val_loss: 0.2194 - val_acc: 0.9556\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0212 - acc: 0.9954 - val_loss: 0.2193 - val_acc: 0.9528\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0160 - acc: 0.9935 - val_loss: 0.1882 - val_acc: 0.9556\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0155 - acc: 0.9954 - val_loss: 0.2358 - val_acc: 0.9556\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0162 - acc: 0.9926 - val_loss: 0.1901 - val_acc: 0.9556\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_50 (Conv1D)          (None, 49, 128)           640       \n",
            "                                                                 \n",
            " max_pooling1d_35 (MaxPoolin  (None, 24, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_51 (Conv1D)          (None, 24, 128)           65664     \n",
            "                                                                 \n",
            " max_pooling1d_36 (MaxPoolin  (None, 12, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_52 (Conv1D)          (None, 12, 128)           65664     \n",
            "                                                                 \n",
            " max_pooling1d_37 (MaxPoolin  (None, 6, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_53 (Conv1D)          (None, 6, 128)            65664     \n",
            "                                                                 \n",
            " global_max_pooling1d_15 (Gl  (None, 128)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 198,019\n",
            "Trainable params: 198,019\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 2s 63ms/step - loss: 1.8169 - acc: 0.7876 - val_loss: 0.2992 - val_acc: 0.9389\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3798 - acc: 0.9332 - val_loss: 0.5756 - val_acc: 0.8194\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2897 - acc: 0.9276 - val_loss: 0.2245 - val_acc: 0.9167\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.1938 - acc: 0.9397 - val_loss: 0.1545 - val_acc: 0.9472\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1390 - acc: 0.9592 - val_loss: 0.1629 - val_acc: 0.9389\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1307 - acc: 0.9601 - val_loss: 0.1804 - val_acc: 0.9361\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.1038 - acc: 0.9647 - val_loss: 0.1397 - val_acc: 0.9500\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0771 - acc: 0.9740 - val_loss: 0.1377 - val_acc: 0.9444\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0678 - acc: 0.9796 - val_loss: 0.1822 - val_acc: 0.9306\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0780 - acc: 0.9787 - val_loss: 0.1238 - val_acc: 0.9667\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0713 - acc: 0.9777 - val_loss: 0.1554 - val_acc: 0.9472\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0610 - acc: 0.9861 - val_loss: 0.1848 - val_acc: 0.9417\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0494 - acc: 0.9842 - val_loss: 0.1340 - val_acc: 0.9583\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0425 - acc: 0.9917 - val_loss: 0.2049 - val_acc: 0.9278\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0438 - acc: 0.9870 - val_loss: 0.1261 - val_acc: 0.9556\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0577 - acc: 0.9814 - val_loss: 0.1936 - val_acc: 0.9389\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0429 - acc: 0.9889 - val_loss: 0.1985 - val_acc: 0.9361\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0395 - acc: 0.9907 - val_loss: 0.1390 - val_acc: 0.9528\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0369 - acc: 0.9917 - val_loss: 0.1550 - val_acc: 0.9528\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0318 - acc: 0.9917 - val_loss: 0.1789 - val_acc: 0.9500\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0345 - acc: 0.9898 - val_loss: 0.1376 - val_acc: 0.9583\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0420 - acc: 0.9907 - val_loss: 0.2591 - val_acc: 0.9167\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0320 - acc: 0.9907 - val_loss: 0.1523 - val_acc: 0.9556\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0299 - acc: 0.9898 - val_loss: 0.1649 - val_acc: 0.9444\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0256 - acc: 0.9926 - val_loss: 0.2070 - val_acc: 0.9361\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0247 - acc: 0.9917 - val_loss: 0.1504 - val_acc: 0.9611\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0287 - acc: 0.9907 - val_loss: 0.2063 - val_acc: 0.9472\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0347 - acc: 0.9926 - val_loss: 0.2280 - val_acc: 0.9389\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0351 - acc: 0.9907 - val_loss: 0.1374 - val_acc: 0.9583\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0231 - acc: 0.9935 - val_loss: 0.2302 - val_acc: 0.9333\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.0264 - acc: 0.9926 - val_loss: 0.1724 - val_acc: 0.9500\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0295 - acc: 0.9917 - val_loss: 0.2368 - val_acc: 0.9306\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0223 - acc: 0.9917 - val_loss: 0.1822 - val_acc: 0.9444\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0270 - acc: 0.9898 - val_loss: 0.1734 - val_acc: 0.9556\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0208 - acc: 0.9926 - val_loss: 0.1797 - val_acc: 0.9472\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0393 - acc: 0.9926 - val_loss: 0.2307 - val_acc: 0.9250\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0301 - acc: 0.9898 - val_loss: 0.1607 - val_acc: 0.9500\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0337 - acc: 0.9917 - val_loss: 0.1678 - val_acc: 0.9444\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0515 - acc: 0.9870 - val_loss: 0.3520 - val_acc: 0.9167\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0712 - acc: 0.9777 - val_loss: 0.1530 - val_acc: 0.9556\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0441 - acc: 0.9852 - val_loss: 0.1582 - val_acc: 0.9417\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0380 - acc: 0.9870 - val_loss: 0.2736 - val_acc: 0.9306\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0357 - acc: 0.9889 - val_loss: 0.1497 - val_acc: 0.9556\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0330 - acc: 0.9898 - val_loss: 0.2544 - val_acc: 0.9278\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0267 - acc: 0.9889 - val_loss: 0.1977 - val_acc: 0.9472\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0256 - acc: 0.9926 - val_loss: 0.2157 - val_acc: 0.9444\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0198 - acc: 0.9944 - val_loss: 0.1914 - val_acc: 0.9500\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0215 - acc: 0.9935 - val_loss: 0.2405 - val_acc: 0.9389\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0189 - acc: 0.9917 - val_loss: 0.1892 - val_acc: 0.9500\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0182 - acc: 0.9954 - val_loss: 0.2125 - val_acc: 0.9444\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0192 - acc: 0.9944 - val_loss: 0.1997 - val_acc: 0.9472\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0192 - acc: 0.9935 - val_loss: 0.1940 - val_acc: 0.9528\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0118 - acc: 0.9954 - val_loss: 0.2175 - val_acc: 0.9417\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0128 - acc: 0.9944 - val_loss: 0.1931 - val_acc: 0.9528\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.2445 - val_acc: 0.9444\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0138 - acc: 0.9944 - val_loss: 0.1978 - val_acc: 0.9528\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0134 - acc: 0.9935 - val_loss: 0.2143 - val_acc: 0.9500\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.0245 - acc: 0.9944 - val_loss: 0.2250 - val_acc: 0.9500\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0237 - acc: 0.9917 - val_loss: 0.1891 - val_acc: 0.9528\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0205 - acc: 0.9944 - val_loss: 0.2584 - val_acc: 0.9361\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0274 - acc: 0.9926 - val_loss: 0.1673 - val_acc: 0.9556\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0216 - acc: 0.9944 - val_loss: 0.1966 - val_acc: 0.9556\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0176 - acc: 0.9944 - val_loss: 0.2291 - val_acc: 0.9472\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.2030 - val_acc: 0.9500\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0169 - acc: 0.9954 - val_loss: 0.2184 - val_acc: 0.9472\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0117 - acc: 0.9944 - val_loss: 0.1937 - val_acc: 0.9556\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.0119 - acc: 0.9935 - val_loss: 0.2050 - val_acc: 0.9528\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0177 - acc: 0.9944 - val_loss: 0.2338 - val_acc: 0.9389\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0109 - acc: 0.9954 - val_loss: 0.1910 - val_acc: 0.9556\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 0.0138 - acc: 0.9944 - val_loss: 0.2318 - val_acc: 0.9500\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0168 - acc: 0.9926 - val_loss: 0.1991 - val_acc: 0.9528\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0144 - acc: 0.9926 - val_loss: 0.2076 - val_acc: 0.9528\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0140 - acc: 0.9917 - val_loss: 0.1989 - val_acc: 0.9500\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0192 - acc: 0.9944 - val_loss: 0.2058 - val_acc: 0.9472\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0123 - acc: 0.9917 - val_loss: 0.2517 - val_acc: 0.9278\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0201 - acc: 0.9954 - val_loss: 0.1998 - val_acc: 0.9472\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0134 - acc: 0.9944 - val_loss: 0.1987 - val_acc: 0.9472\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0114 - acc: 0.9917 - val_loss: 0.2056 - val_acc: 0.9500\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0148 - acc: 0.9954 - val_loss: 0.2058 - val_acc: 0.9500\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0104 - acc: 0.9944 - val_loss: 0.2162 - val_acc: 0.9417\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0128 - acc: 0.9944 - val_loss: 0.2020 - val_acc: 0.9528\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0110 - acc: 0.9963 - val_loss: 0.2243 - val_acc: 0.9472\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0129 - acc: 0.9944 - val_loss: 0.1947 - val_acc: 0.9556\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0172 - acc: 0.9926 - val_loss: 0.2263 - val_acc: 0.9417\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0193 - acc: 0.9963 - val_loss: 0.2205 - val_acc: 0.9389\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0166 - acc: 0.9954 - val_loss: 0.1715 - val_acc: 0.9528\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0102 - acc: 0.9944 - val_loss: 0.2331 - val_acc: 0.9472\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0158 - acc: 0.9935 - val_loss: 0.2186 - val_acc: 0.9472\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0109 - acc: 0.9944 - val_loss: 0.2263 - val_acc: 0.9417\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0151 - acc: 0.9944 - val_loss: 0.2408 - val_acc: 0.9444\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0185 - acc: 0.9954 - val_loss: 0.2087 - val_acc: 0.9528\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0133 - acc: 0.9944 - val_loss: 0.2134 - val_acc: 0.9472\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0106 - acc: 0.9935 - val_loss: 0.2096 - val_acc: 0.9500\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0176 - acc: 0.9926 - val_loss: 0.2223 - val_acc: 0.9472\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.0126 - acc: 0.9954 - val_loss: 0.2125 - val_acc: 0.9500\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0150 - acc: 0.9926 - val_loss: 0.2152 - val_acc: 0.9500\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.2436 - val_acc: 0.9444\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0125 - acc: 0.9935 - val_loss: 0.2048 - val_acc: 0.9500\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0131 - acc: 0.9935 - val_loss: 0.2150 - val_acc: 0.9528\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0109 - acc: 0.9944 - val_loss: 0.2175 - val_acc: 0.9528\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_54 (Conv1D)          (None, 49, 128)           640       \n",
            "                                                                 \n",
            " max_pooling1d_38 (MaxPoolin  (None, 24, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_55 (Conv1D)          (None, 24, 128)           65664     \n",
            "                                                                 \n",
            " max_pooling1d_39 (MaxPoolin  (None, 12, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_56 (Conv1D)          (None, 12, 128)           65664     \n",
            "                                                                 \n",
            " max_pooling1d_40 (MaxPoolin  (None, 6, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_57 (Conv1D)          (None, 6, 128)            65664     \n",
            "                                                                 \n",
            " max_pooling1d_41 (MaxPoolin  (None, 3, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_58 (Conv1D)          (None, 3, 128)            65664     \n",
            "                                                                 \n",
            " global_max_pooling1d_16 (Gl  (None, 128)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 263,683\n",
            "Trainable params: 263,683\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 2s 60ms/step - loss: 1.6047 - acc: 0.7829 - val_loss: 0.2958 - val_acc: 0.9194\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.4464 - acc: 0.9109 - val_loss: 0.1976 - val_acc: 0.9333\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2237 - acc: 0.9314 - val_loss: 0.2056 - val_acc: 0.9222\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1785 - acc: 0.9341 - val_loss: 0.1738 - val_acc: 0.9361\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1482 - acc: 0.9508 - val_loss: 0.1456 - val_acc: 0.9500\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1261 - acc: 0.9601 - val_loss: 0.1382 - val_acc: 0.9583\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.1182 - acc: 0.9666 - val_loss: 0.1335 - val_acc: 0.9583\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1048 - acc: 0.9629 - val_loss: 0.1382 - val_acc: 0.9556\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0923 - acc: 0.9694 - val_loss: 0.1525 - val_acc: 0.9472\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0906 - acc: 0.9768 - val_loss: 0.1885 - val_acc: 0.9472\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0785 - acc: 0.9777 - val_loss: 0.1548 - val_acc: 0.9389\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0666 - acc: 0.9824 - val_loss: 0.1520 - val_acc: 0.9417\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0579 - acc: 0.9833 - val_loss: 0.1618 - val_acc: 0.9556\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0540 - acc: 0.9870 - val_loss: 0.1892 - val_acc: 0.9306\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0437 - acc: 0.9889 - val_loss: 0.1586 - val_acc: 0.9389\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.0413 - acc: 0.9870 - val_loss: 0.1767 - val_acc: 0.9500\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0423 - acc: 0.9870 - val_loss: 0.1738 - val_acc: 0.9500\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0319 - acc: 0.9917 - val_loss: 0.1678 - val_acc: 0.9444\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0323 - acc: 0.9917 - val_loss: 0.1991 - val_acc: 0.9417\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0327 - acc: 0.9907 - val_loss: 0.1846 - val_acc: 0.9528\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0358 - acc: 0.9898 - val_loss: 0.2974 - val_acc: 0.9278\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0351 - acc: 0.9898 - val_loss: 0.2591 - val_acc: 0.9361\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.0462 - acc: 0.9889 - val_loss: 0.1526 - val_acc: 0.9639\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.0415 - acc: 0.9889 - val_loss: 0.1877 - val_acc: 0.9500\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0304 - acc: 0.9917 - val_loss: 0.2038 - val_acc: 0.9417\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.0287 - acc: 0.9898 - val_loss: 0.2782 - val_acc: 0.9333\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0263 - acc: 0.9926 - val_loss: 0.1703 - val_acc: 0.9611\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0338 - acc: 0.9889 - val_loss: 0.2328 - val_acc: 0.9417\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0291 - acc: 0.9898 - val_loss: 0.2582 - val_acc: 0.9361\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0240 - acc: 0.9907 - val_loss: 0.1667 - val_acc: 0.9583\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0280 - acc: 0.9926 - val_loss: 0.2502 - val_acc: 0.9444\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.2339 - val_acc: 0.9444\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0209 - acc: 0.9907 - val_loss: 0.2355 - val_acc: 0.9472\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0220 - acc: 0.9917 - val_loss: 0.2470 - val_acc: 0.9417\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0179 - acc: 0.9917 - val_loss: 0.2312 - val_acc: 0.9500\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0187 - acc: 0.9926 - val_loss: 0.2432 - val_acc: 0.9444\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0258 - acc: 0.9898 - val_loss: 0.2606 - val_acc: 0.9361\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0210 - acc: 0.9898 - val_loss: 0.3216 - val_acc: 0.9444\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0321 - acc: 0.9898 - val_loss: 0.2556 - val_acc: 0.9389\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0349 - acc: 0.9889 - val_loss: 0.1752 - val_acc: 0.9583\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0353 - acc: 0.9861 - val_loss: 0.2207 - val_acc: 0.9500\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0432 - acc: 0.9889 - val_loss: 0.3145 - val_acc: 0.9278\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0466 - acc: 0.9833 - val_loss: 0.2002 - val_acc: 0.9444\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0208 - acc: 0.9944 - val_loss: 0.2136 - val_acc: 0.9583\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0194 - acc: 0.9926 - val_loss: 0.2122 - val_acc: 0.9528\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.0174 - acc: 0.9926 - val_loss: 0.2598 - val_acc: 0.9417\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0152 - acc: 0.9935 - val_loss: 0.1905 - val_acc: 0.9528\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0153 - acc: 0.9935 - val_loss: 0.2507 - val_acc: 0.9472\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0120 - acc: 0.9935 - val_loss: 0.2626 - val_acc: 0.9472\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0144 - acc: 0.9954 - val_loss: 0.2730 - val_acc: 0.9472\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0136 - acc: 0.9935 - val_loss: 0.2665 - val_acc: 0.9500\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0147 - acc: 0.9926 - val_loss: 0.2323 - val_acc: 0.9528\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0159 - acc: 0.9907 - val_loss: 0.2323 - val_acc: 0.9500\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0136 - acc: 0.9944 - val_loss: 0.2684 - val_acc: 0.9389\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0138 - acc: 0.9935 - val_loss: 0.2717 - val_acc: 0.9389\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0139 - acc: 0.9944 - val_loss: 0.2416 - val_acc: 0.9444\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0121 - acc: 0.9935 - val_loss: 0.2852 - val_acc: 0.9444\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0156 - acc: 0.9935 - val_loss: 0.2461 - val_acc: 0.9417\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.2411 - val_acc: 0.9472\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.0093 - acc: 0.9935 - val_loss: 0.2825 - val_acc: 0.9472\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0143 - acc: 0.9944 - val_loss: 0.2232 - val_acc: 0.9444\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0167 - acc: 0.9944 - val_loss: 0.2681 - val_acc: 0.9500\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.0134 - acc: 0.9926 - val_loss: 0.2844 - val_acc: 0.9417\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0105 - acc: 0.9944 - val_loss: 0.2471 - val_acc: 0.9472\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0103 - acc: 0.9944 - val_loss: 0.2800 - val_acc: 0.9444\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0096 - acc: 0.9935 - val_loss: 0.2646 - val_acc: 0.9472\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0097 - acc: 0.9954 - val_loss: 0.2926 - val_acc: 0.9417\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0094 - acc: 0.9935 - val_loss: 0.3014 - val_acc: 0.9417\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0093 - acc: 0.9926 - val_loss: 0.2973 - val_acc: 0.9417\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0114 - acc: 0.9944 - val_loss: 0.2970 - val_acc: 0.9417\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0110 - acc: 0.9935 - val_loss: 0.2884 - val_acc: 0.9472\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0124 - acc: 0.9954 - val_loss: 0.2893 - val_acc: 0.9472\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0130 - acc: 0.9944 - val_loss: 0.3289 - val_acc: 0.9333\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0161 - acc: 0.9917 - val_loss: 0.2375 - val_acc: 0.9472\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0161 - acc: 0.9926 - val_loss: 0.2545 - val_acc: 0.9417\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0157 - acc: 0.9935 - val_loss: 0.2317 - val_acc: 0.9528\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0122 - acc: 0.9944 - val_loss: 0.3437 - val_acc: 0.9417\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0139 - acc: 0.9926 - val_loss: 0.1966 - val_acc: 0.9556\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0117 - acc: 0.9944 - val_loss: 0.2456 - val_acc: 0.9417\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.0149 - acc: 0.9917 - val_loss: 0.2141 - val_acc: 0.9500\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0160 - acc: 0.9935 - val_loss: 0.2939 - val_acc: 0.9444\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 38ms/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.3006 - val_acc: 0.9500\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.0129 - acc: 0.9935 - val_loss: 0.2379 - val_acc: 0.9528\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0140 - acc: 0.9944 - val_loss: 0.2774 - val_acc: 0.9472\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.0148 - acc: 0.9917 - val_loss: 0.2726 - val_acc: 0.9472\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.0115 - acc: 0.9944 - val_loss: 0.2787 - val_acc: 0.9472\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.0108 - acc: 0.9935 - val_loss: 0.2734 - val_acc: 0.9472\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.0113 - acc: 0.9954 - val_loss: 0.2760 - val_acc: 0.9472\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 0.0108 - acc: 0.9926 - val_loss: 0.2444 - val_acc: 0.9500\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.0114 - acc: 0.9926 - val_loss: 0.2799 - val_acc: 0.9472\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0102 - acc: 0.9944 - val_loss: 0.2796 - val_acc: 0.9472\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0098 - acc: 0.9935 - val_loss: 0.2480 - val_acc: 0.9528\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0128 - acc: 0.9935 - val_loss: 0.2761 - val_acc: 0.9472\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0104 - acc: 0.9944 - val_loss: 0.3072 - val_acc: 0.9417\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0106 - acc: 0.9954 - val_loss: 0.2288 - val_acc: 0.9528\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.0163 - acc: 0.9944 - val_loss: 0.2766 - val_acc: 0.9472\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.0101 - acc: 0.9954 - val_loss: 0.2991 - val_acc: 0.9472\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0123 - acc: 0.9917 - val_loss: 0.2580 - val_acc: 0.9417\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0120 - acc: 0.9935 - val_loss: 0.2589 - val_acc: 0.9417\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.0087 - acc: 0.9944 - val_loss: 0.2866 - val_acc: 0.9417\n"
          ]
        }
      ],
      "source": [
        "arr = []\n",
        "for number in range(5):\n",
        "    model = Sequential()\n",
        "    y_train = tf.keras.utils.to_categorical(y1_train, 3)\n",
        "    y_test = tf.keras.utils.to_categorical(y1_test, 3)\n",
        "    model.add(layers.Conv1D(128, 4, activation='relu', padding='same', input_shape=(X1.shape[1], 1)))\n",
        "    for i in range(number):\n",
        "        model.add(layers.MaxPooling1D(2))\n",
        "        model.add(layers.Conv1D(128, 4, activation='relu', padding='same'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(3, activation=\"softmax\"))\n",
        "    model.summary()\n",
        "    # Компиляция модели\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    # Обучение модели\n",
        "    m=model.fit(X1_train, y_train, validation_data=(X1_test, y_test), epochs=100, batch_size=128)\n",
        "    arr.append(np.max(m.history[\"val_acc\"]))\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vW_w-DKunUyR",
        "outputId": "a2939b80-b780-4d8f-f2ee-1b80ca41d6a9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9305555820465088, 0.9472222328186035, 0.9611111283302307, 0.9666666388511658, 0.9638888835906982]\n"
          ]
        }
      ],
      "source": [
        "print(arr)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YoW39_rynUyS",
        "outputId": "492a9ddf-86a1-4a7b-db41-2b31ebbef506"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем вывод что модель с 3 слоями Conv1D показала наилучший результат по val_acc\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FxijaIb1nUyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 32  64  96 128 160 192 224 256]\n"
          ]
        }
      ],
      "source": [
        "ns = np.linspace(32,256,8,dtype=int)\n",
        "print(ns)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DOJFAiG9nUyS",
        "outputId": "dba37050-3250-4f21-b1c8-044a5d815024"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_61 (Conv1D)          (None, 49, 32)            160       \n",
            "                                                                 \n",
            " max_pooling1d_40 (MaxPoolin  (None, 24, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_62 (Conv1D)          (None, 24, 32)            4128      \n",
            "                                                                 \n",
            " max_pooling1d_41 (MaxPoolin  (None, 12, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_63 (Conv1D)          (None, 12, 32)            4128      \n",
            "                                                                 \n",
            " global_max_pooling1d_20 (Gl  (None, 32)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,515\n",
            "Trainable params: 8,515\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_64 (Conv1D)          (None, 49, 64)            320       \n",
            "                                                                 \n",
            " max_pooling1d_42 (MaxPoolin  (None, 24, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_65 (Conv1D)          (None, 24, 64)            16448     \n",
            "                                                                 \n",
            " max_pooling1d_43 (MaxPoolin  (None, 12, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_66 (Conv1D)          (None, 12, 64)            16448     \n",
            "                                                                 \n",
            " global_max_pooling1d_21 (Gl  (None, 64)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,411\n",
            "Trainable params: 33,411\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_67 (Conv1D)          (None, 49, 96)            480       \n",
            "                                                                 \n",
            " max_pooling1d_44 (MaxPoolin  (None, 24, 96)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_68 (Conv1D)          (None, 24, 96)            36960     \n",
            "                                                                 \n",
            " max_pooling1d_45 (MaxPoolin  (None, 12, 96)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_69 (Conv1D)          (None, 12, 96)            36960     \n",
            "                                                                 \n",
            " global_max_pooling1d_22 (Gl  (None, 96)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 3)                 291       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74,691\n",
            "Trainable params: 74,691\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_70 (Conv1D)          (None, 49, 128)           640       \n",
            "                                                                 \n",
            " max_pooling1d_46 (MaxPoolin  (None, 24, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_71 (Conv1D)          (None, 24, 128)           65664     \n",
            "                                                                 \n",
            " max_pooling1d_47 (MaxPoolin  (None, 12, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_72 (Conv1D)          (None, 12, 128)           65664     \n",
            "                                                                 \n",
            " global_max_pooling1d_23 (Gl  (None, 128)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132,355\n",
            "Trainable params: 132,355\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_73 (Conv1D)          (None, 49, 160)           800       \n",
            "                                                                 \n",
            " max_pooling1d_48 (MaxPoolin  (None, 24, 160)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_74 (Conv1D)          (None, 24, 160)           102560    \n",
            "                                                                 \n",
            " max_pooling1d_49 (MaxPoolin  (None, 12, 160)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_75 (Conv1D)          (None, 12, 160)           102560    \n",
            "                                                                 \n",
            " global_max_pooling1d_24 (Gl  (None, 160)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 3)                 483       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206,403\n",
            "Trainable params: 206,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_76 (Conv1D)          (None, 49, 192)           960       \n",
            "                                                                 \n",
            " max_pooling1d_50 (MaxPoolin  (None, 24, 192)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_77 (Conv1D)          (None, 24, 192)           147648    \n",
            "                                                                 \n",
            " max_pooling1d_51 (MaxPoolin  (None, 12, 192)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_78 (Conv1D)          (None, 12, 192)           147648    \n",
            "                                                                 \n",
            " global_max_pooling1d_25 (Gl  (None, 192)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 3)                 579       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 296,835\n",
            "Trainable params: 296,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_79 (Conv1D)          (None, 49, 224)           1120      \n",
            "                                                                 \n",
            " max_pooling1d_52 (MaxPoolin  (None, 24, 224)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_80 (Conv1D)          (None, 24, 224)           200928    \n",
            "                                                                 \n",
            " max_pooling1d_53 (MaxPoolin  (None, 12, 224)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_81 (Conv1D)          (None, 12, 224)           200928    \n",
            "                                                                 \n",
            " global_max_pooling1d_26 (Gl  (None, 224)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 3)                 675       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 403,651\n",
            "Trainable params: 403,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_82 (Conv1D)          (None, 49, 256)           1280      \n",
            "                                                                 \n",
            " max_pooling1d_54 (MaxPoolin  (None, 24, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_83 (Conv1D)          (None, 24, 256)           262400    \n",
            "                                                                 \n",
            " max_pooling1d_55 (MaxPoolin  (None, 12, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_84 (Conv1D)          (None, 12, 256)           262400    \n",
            "                                                                 \n",
            " global_max_pooling1d_27 (Gl  (None, 256)              0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 526,851\n",
            "Trainable params: 526,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[0.9527778029441833, 0.9555555582046509, 0.9694444537162781, 0.9638888835906982, 0.9583333134651184, 0.9583333134651184, 0.9722222089767456, 0.9638888835906982]\n"
          ]
        }
      ],
      "source": [
        "arr = []\n",
        "for number in ns:\n",
        "    model = Sequential()\n",
        "    y_train = tf.keras.utils.to_categorical(y1_train, 3)\n",
        "    y_test = tf.keras.utils.to_categorical(y1_test, 3)\n",
        "    model.add(layers.Conv1D(number, 4, activation='relu', padding='same', input_shape=(X1.shape[1], 1)))\n",
        "    model.add(layers.MaxPooling1D(2))\n",
        "    model.add(layers.Conv1D(number, 4, activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling1D(2))\n",
        "    model.add(layers.Conv1D(number, 4, activation='relu', padding='same'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(3, activation=\"softmax\"))\n",
        "    model.summary()\n",
        "    # Компиляция модели\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    # Обучение модели\n",
        "    m=model.fit(X1_train, y_train, validation_data=(X1_test, y_test), epochs=100, batch_size=128, verbose=0)\n",
        "    arr.append(np.max(m.history[\"val_acc\"]))\n",
        "print(arr)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MR4u5YMXnUyS",
        "outputId": "4a2b8197-acd4-442d-d152-3d28d08b62ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9527778029441833, 0.9555555582046509, 0.9694444537162781, 0.9638888835906982, 0.9583333134651184, 0.9583333134651184, 0.9722222089767456, 0.9638888835906982]\n"
          ]
        }
      ],
      "source": [
        "print(arr)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MIh1gZxznUyT",
        "outputId": "938d46d1-8310-4b97-97ad-484febf8e3fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем вывод что модель с 3 слоями Conv1D показала наилучший результат c 224 нейронами и 3 слоями conv1D"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jtdMh6BknUyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mEpESDmSnUyT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}